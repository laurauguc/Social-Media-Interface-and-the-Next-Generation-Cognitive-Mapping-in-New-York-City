{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Intro\n",
    "\n",
    "Natural Language Tool-Kit\n",
    "\n",
    "Sentiment Analysis company: Sentdex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms\n",
    "\n",
    "* Tokenizing - grouping things: word tokenizers (seperate by words); sentence tokenizers (seperate by sentences).\n",
    "\n",
    "* Lexicon - words and their meaning/value\n",
    "\n",
    "* Corpora - boy of text. ex: medical journals, presidential speeches, English language\n",
    "\n",
    "# Text Pre-Processing\n",
    "\n",
    "## Tokenizing Words and Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_text = \"Hello Mr. Smith, how are you today? The weather is great and Python is awesome. The sky is pinkish-blue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you today?', 'The weather is great and Python is awesome.', 'The sky is pinkish-blue']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr.\n",
      "Smith\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "today\n",
      "?\n",
      "The\n",
      "weather\n",
      "is\n",
      "great\n",
      "and\n",
      "Python\n",
      "is\n",
      "awesome\n",
      ".\n",
      "The\n",
      "sky\n",
      "is\n",
      "pinkish-blue\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(example_text):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced tokenizers can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union # state of the union addresses\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(sample_text) # training on this text\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            \n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# process_content()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "touple of word and part of speech (POS) tags:\n",
    "\n",
    "__POS tag list:__\n",
    "\n",
    "CC\tcoordinating conjunction\n",
    "\n",
    "CD\tcardinal digit\n",
    "\n",
    "DT\tdeterminer\n",
    "\n",
    "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "\n",
    "FW\tforeign word\n",
    "\n",
    "IN\tpreposition/subordinating conjunction\n",
    "\n",
    "JJ\tadjective\t'big'\n",
    "\n",
    "JJR\tadjective, comparative\t'bigger'\n",
    "\n",
    "JJS\tadjective, superlative\t'biggest'\n",
    "\n",
    "LS\tlist marker\t1)\n",
    "\n",
    "MD\tmodal\tcould, will\n",
    "\n",
    "NN\tnoun, singular 'desk'\n",
    "\n",
    "NNS\tnoun plural\t'desks'\n",
    "\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "\n",
    "PDT\tpredeterminer\t'all the kids'\n",
    "\n",
    "POS\tpossessive ending\tparent's\n",
    "\n",
    "PRP\tpersonal pronoun\tI, he, she\n",
    "\n",
    "PRP\\$\tpossessive pronoun\tmy, his, hers\n",
    "\n",
    "RB\tadverb\tvery, silently,\n",
    "\n",
    "RBR\tadverb, comparative\tbetter\n",
    "\n",
    "RBS\tadverb, superlative\tbest\n",
    "\n",
    "RP\tparticle\tgive up\n",
    "\n",
    "TO\tto\tgo 'to' the store.\n",
    "\n",
    "UH\tinterjection\terrrrrrrrm\n",
    "\n",
    "VB\tverb, base form\ttake\n",
    "\n",
    "VBD\tverb, past tense\ttook\n",
    "\n",
    "VBG\tverb, gerund/present participle\ttaking\n",
    "\n",
    "VBN\tverb, past participle\ttaken\n",
    "\n",
    "VBP\tverb, sing. present, non-3d\ttake\n",
    "\n",
    "VBZ\tverb, 3rd person sing. present\ttakes\n",
    "\n",
    "WDT\twh-determiner\twhich\n",
    "\n",
    "WP\twh-pronoun\twho, what\n",
    "\n",
    "WP\\$\tpossessive wh-pronoun\twhose\n",
    "\n",
    "WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can lead to problems with Twitter texts: person's name in lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 (to name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "\n",
    "Similar to stemming but result in a real word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plant\n",
      "plant\n",
      " \n",
      "better\n",
      "good\n",
      "better\n",
      " \n",
      "eating\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "print(lm().lemmatize(\"plants\"))\n",
    "print(lm().lemmatize(\"plant\"), end = \"\\n \\n\")\n",
    "\n",
    "print(lm().lemmatize(\"better\"))\n",
    "print(lm().lemmatize(\"better\", pos = \"a\"))\n",
    "print(lm().lemmatize(\"better\", pos = \"v\"), end = \"\\n \\n\")\n",
    "\n",
    "print(lm().lemmatize(\"eating\"), end = \"\\n \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"wordnet\")\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "syns = wn.synsets(\"program\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('plan.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(syns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].lemmas()[0].name()) # just the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plan.n.01\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].name()) # synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a series of steps to be carried out or goals to be accomplished\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].definition()) # definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
     ]
    }
   ],
   "source": [
    "print(syns[0].examples()) # examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parking_area', 'ballpark', 'Mungo_Park', 'common', 'parkland', 'car_park', 'parking_lot', 'green', 'Park', 'commons', 'park'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wn.synsets(\"park\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Semantic Symilarity\n",
    "w1 = wn.synset(\"tree.n.01\")\n",
    "w2 = wn.synset(\"leaf.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2)) # Wu & Palmer (1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "# Semantic Symilarity\n",
    "w1 = wn.synset(\"boat.n.01\")\n",
    "w2 = wn.synset(\"ship.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2)) # Wu & Palmer (1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Semantic Symilarity\n",
    "w1 = wn.synset(\"tree.n.01\")\n",
    "w2 = wn.synset(\"plant.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2)) # Wu & Palmer (1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Semantic Symilarity\n",
    "w1 = wn.synset(\"tree.n.01\")\n",
    "w2 = wn.synset(\"cat.n.01\")\n",
    "\n",
    "print(w1.wup_similarity(w2)) # Wu & Palmer (1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8235294117647058\n"
     ]
    }
   ],
   "source": [
    "# Semantic Symilarity\n",
    "w1 = wn.synset(\"tree.n.01\")\n",
    "w2 = wn.synset(\"plant.n.02\")\n",
    "\n",
    "print(w1.wup_similarity(w2)) # Wu & Palmer (1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "# Semantic Symilarity\n",
    "w1 = wn.synset(\"tree.n.01\")\n",
    "w2 = wn.synset(\"plant.n.03\")\n",
    "\n",
    "print(w1.wup_similarity(w2)) # Wu & Palmer (1994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classifier for text analysis.\n",
    "\n",
    "features of documents are words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# documents = [(list(movie_reviews.words(fileid)), category)\n",
    "#            for category in movie_reviews.categories()\n",
    "#            for fileid in movie_reviews.fileids(category)] # list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# same as above but easier to read\n",
    "\n",
    "documents = []\n",
    "\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append((list(movie_reviews.words(fileid)), category))\n",
    "        \n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', 'review', 'damn', 'that', 'y2k', 'bug', '.', 'it', \"'\", 's', 'got', 'a', 'head', 'start', 'in', 'this', 'movie', 'starring', 'jamie', 'lee', 'curtis', 'and', 'another', 'baldwin', 'brother', '(', 'william', 'this', 'time', ')', 'in', 'a', 'story', 'regarding', 'a', 'crew', 'of', 'a', 'tugboat', 'that', 'comes', 'across', 'a', 'deserted', 'russian', 'tech', 'ship', 'that', 'has', 'a', 'strangeness', 'to', 'it', 'when', 'they', 'kick', 'the', 'power', 'back', 'on', '.', 'little', 'do', 'they', 'know', 'the', 'power', 'within', '.', '.', '.', 'going', 'for', 'the', 'gore', 'and', 'bringing', 'on', 'a', 'few', 'action', 'sequences', 'here', 'and', 'there', ',', 'virus', 'still', 'feels', 'very', 'empty', ',', 'like', 'a', 'movie', 'going', 'for', 'all', 'flash', 'and', 'no', 'substance', '.', 'we', 'don', \"'\", 't', 'know', 'why', 'the', 'crew', 'was', 'really', 'out', 'in', 'the', 'middle', 'of', 'nowhere', ',', 'we', 'don', \"'\", 't', 'know', 'the', 'origin', 'of', 'what', 'took', 'over', 'the', 'ship', '(', 'just', 'that', 'a', 'big', 'pink', 'flashy', 'thing', 'hit', 'the', 'mir', ')', ',', 'and', ',', 'of', 'course', ',', 'we', 'don', \"'\", 't', 'know', 'why', 'donald', 'sutherland', 'is', 'stumbling', 'around', 'drunkenly', 'throughout', '.', 'here', ',', 'it', \"'\", 's', 'just', '\"', 'hey', ',', 'let', \"'\", 's', 'chase', 'these', 'people', 'around', 'with', 'some', 'robots', '\"', '.', 'the', 'acting', 'is', 'below', 'average', ',', 'even', 'from', 'the', 'likes', 'of', 'curtis', '.', 'you', \"'\", 're', 'more', 'likely', 'to', 'get', 'a', 'kick', 'out', 'of', 'her', 'work', 'in', 'halloween', 'h20', '.', 'sutherland', 'is', 'wasted', 'and', 'baldwin', ',', 'well', ',', 'he', \"'\", 's', 'acting', 'like', 'a', 'baldwin', ',', 'of', 'course', '.', 'the', 'real', 'star', 'here', 'are', 'stan', 'winston', \"'\", 's', 'robot', 'design', ',', 'some', 'schnazzy', 'cgi', ',', 'and', 'the', 'occasional', 'good', 'gore', 'shot', ',', 'like', 'picking', 'into', 'someone', \"'\", 's', 'brain', '.', 'so', ',', 'if', 'robots', 'and', 'body', 'parts', 'really', 'turn', 'you', 'on', ',', 'here', \"'\", 's', 'your', 'movie', '.', 'otherwise', ',', 'it', \"'\", 's', 'pretty', 'much', 'a', 'sunken', 'ship', 'of', 'a', 'movie', '.'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595)]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n"
     ]
    }
   ],
   "source": [
    "print(all_words[\"stupid\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertig Words to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86400"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60*60*24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word = \"plant\" # try vegetations (select the correct meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('plant.n.01'),\n",
       " Synset('plant.n.02'),\n",
       " Synset('plant.n.03'),\n",
       " Synset('plant.n.04'),\n",
       " Synset('plant.v.01'),\n",
       " Synset('implant.v.01'),\n",
       " Synset('establish.v.02'),\n",
       " Synset('plant.v.04'),\n",
       " Synset('plant.v.05'),\n",
       " Synset('plant.v.06')]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_w = wn.synsets(word)\n",
    "synset_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ) buildings for carrying on industrial labor\n",
      "1 ) (botany) a living organism lacking the power of locomotion\n",
      "2 ) an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience\n",
      "3 ) something planted secretly for discovery by another\n",
      "4 ) put or set (seeds, seedlings, or plants) into the ground\n",
      "5 ) fix or set securely or deeply\n",
      "6 ) set up or lay the groundwork for\n",
      "7 ) place into a river\n",
      "8 ) place something or someone in a certain position in order to secretly observe or deceive\n",
      "9 ) put firmly in the mind\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(synset_w)):\n",
    "    print(i, \")\", wn.synsets(word)[i].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# meaning one is of interst. Now find hyponyms.\n",
    "\n",
    "word = wn.synsets(word)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('acrogen.n.01'),\n",
       " Synset('air_plant.n.01'),\n",
       " Synset('annual.n.01'),\n",
       " Synset('apomict.n.01'),\n",
       " Synset('aquatic.n.01')]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# level 1 hyponyms\n",
    "\n",
    "hyponyms_1 = word.hyponyms()\n",
    "hyponyms_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [Synset('aeschynanthus.n.01'),\n",
       "  Synset('hemiepiphyte.n.01'),\n",
       "  Synset('spanish_moss.n.01'),\n",
       "  Synset('strangler.n.01'),\n",
       "  Synset('waxflower.n.02')],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# level 2 hyponyms\n",
    "\n",
    "hyponyms_2 = []\n",
    "\n",
    "for i in range(len(hyponyms_1)):\n",
    "    hyponyms_2.append(hyponyms_1[i].hyponyms())\n",
    "    \n",
    "hyponyms_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flattened list\n",
    "hyponyms_2 = [y for x in hyponyms_2 for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('lipstick_plant.n.01'),\n",
       " Synset('pitch_apple.n.01'),\n",
       " Synset('field_corn.n.01'),\n",
       " Synset('flamingo_flower.n.01'),\n",
       " Synset('canterbury_bell.n.01')]"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyponyms_3 = []\n",
    "\n",
    "for i in range(len(hyponyms_2)):\n",
    "    hyponyms_3.append(hyponyms_2[i].hyponyms())\n",
    "    \n",
    "hyponyms_3 = [y for x in hyponyms_3 for y in x]\n",
    "\n",
    "hyponyms_3[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dent_corn.n.01'),\n",
       " Synset('flint_corn.n.01'),\n",
       " Synset('soft_corn.n.01'),\n",
       " Synset('green_arrow_arum.n.01'),\n",
       " Synset('common_duckweed.n.01')]"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyponyms_4 = []\n",
    "\n",
    "for i in range(len(hyponyms_3)):\n",
    "    hyponyms_4.append(hyponyms_3[i].hyponyms())\n",
    "\n",
    "hyponyms_4 = [y for x in hyponyms_4 for y in x]    \n",
    "    \n",
    "hyponyms_4[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyponyms_all = []\n",
    "\n",
    "hyponyms_all.append(hyponyms_1)\n",
    "hyponyms_all.append(hyponyms_2)\n",
    "hyponyms_all.append(hyponyms_3)\n",
    "hyponyms_all.append(hyponyms_4)\n",
    "\n",
    "hyponyms_all = [y for x in hyponyms_all for y in x]  \n",
    "\n",
    "len(hyponyms_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hyp_current = list(hyponyms_1)\n",
    "hyp_lower = list()\n",
    "hyp_all = list(hyponyms_1)\n",
    "\n",
    "for level in range(3): # 4 levels (3 + initial one)\n",
    "    for i in range(len(hyp_current)):\n",
    "        hyp_lower.append(hyp_current[i].hyponyms())\n",
    "        \n",
    "    hyp_current = list(y for x in hyp_lower for y in x)\n",
    "    hyp_all.extend(hyp_current)\n",
    "    hyp_lower = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2096"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hyp_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all hyponyms function (8 steps in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_hyponyms(word, meaning_n):\n",
    "    word = wn.synsets(word)[meaning_n]\n",
    "    hyp_current = word.hyponyms()\n",
    "    hyp_all = list(hyp_current)\n",
    "    hyp_lower = []\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    while count < len(hyp_all):  \n",
    "        count = len(hyp_all)\n",
    "        for i in range(len(hyp_current)):\n",
    "            hyp_lower.append(hyp_current[i].hyponyms())\n",
    "        \n",
    "        hyp_current = list(y for x in hyp_lower for y in x)\n",
    "        hyp_all.extend(hyp_current)\n",
    "        hyp_lower = list()    \n",
    "    \n",
    "    return hyp_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Meronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_meronyms(word, meaning_n):\n",
    "    word = wn.synsets(word)[meaning_n]\n",
    "    mer_current = word.part_meronyms()\n",
    "    mer_all = list(mer_current)\n",
    "    mer_lower = []\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    while count < len(mer_all):  \n",
    "        count = len(mer_all)\n",
    "        for i in range(len(mer_current)):\n",
    "            mer_lower.append(mer_current[i].part_meronyms())\n",
    "        \n",
    "        mer_current = list(y for x in mer_lower for y in x)\n",
    "        mer_all.extend(mer_current)\n",
    "        mer_lower = list()    \n",
    "    \n",
    "    return mer_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find hyponyms of meronyms and meronyms of hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_meronyms_hyponyms(word, meaning_n):\n",
    "    word = wn.synsets(word)[meaning_n]\n",
    "    current = word.part_meronyms()\n",
    "    current.extend(word.hyponyms())\n",
    "    \n",
    "    lower = []\n",
    "    all = list(current)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    while count < len(all):\n",
    "        count = len(all)\n",
    "        for i in range(len(current)):\n",
    "            lower.append(current[i].part_meronyms())\n",
    "            lower.append(current[i].hyponyms())\n",
    "        \n",
    "        current = list(y for x in lower for y in x)\n",
    "        all.extend(current)\n",
    "        lower = list()\n",
    "            \n",
    "    return all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select correct meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To find meaning number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ) buildings for carrying on industrial labor\n",
      "1 ) (botany) a living organism lacking the power of locomotion\n",
      "2 ) an actor situated in the audience whose acting is rehearsed but seems spontaneous to the audience\n",
      "3 ) something planted secretly for discovery by another\n",
      "4 ) put or set (seeds, seedlings, or plants) into the ground\n",
      "5 ) fix or set securely or deeply\n",
      "6 ) set up or lay the groundwork for\n",
      "7 ) place into a river\n",
      "8 ) place something or someone in a certain position in order to secretly observe or deceive\n",
      "9 ) put firmly in the mind\n"
     ]
    }
   ],
   "source": [
    "word = \"plant\"\n",
    "\n",
    "synset_w = wn.synsets(word)\n",
    "\n",
    "for i in range(len(synset_w)):\n",
    "    print(i, \")\", wn.synsets(word)[i].definition())\n",
    "    \n",
    "meaning_n = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('organism.n.01')]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word = wn.synsets(word)[meaning_n]\n",
    "word.hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4699"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_hyponyms(\"plant\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_meronyms(\"plant\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7723"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_meronyms_hyponyms(\"plant\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('organism.n.01')]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('plant.n.02')"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__arguments search__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ',\n",
       " 'ADJ_SAT',\n",
       " 'ADV',\n",
       " 'MORPHOLOGICAL_SUBSTITUTIONS',\n",
       " 'NOUN',\n",
       " 'VERB',\n",
       " '_ENCODING',\n",
       " '_FILEMAP',\n",
       " '_FILES',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_compute_max_depth',\n",
       " '_data_file',\n",
       " '_data_file_map',\n",
       " '_encoding',\n",
       " '_exception_map',\n",
       " '_fileids',\n",
       " '_get_root',\n",
       " '_key_count_file',\n",
       " '_key_synset_file',\n",
       " '_lang_data',\n",
       " '_lemma_pos_offset_map',\n",
       " '_lexnames',\n",
       " '_load_exception_map',\n",
       " '_load_lang_data',\n",
       " '_load_lemma_pos_offset_map',\n",
       " '_max_depth',\n",
       " '_morphy',\n",
       " '_omw_reader',\n",
       " '_pos_names',\n",
       " '_pos_numbers',\n",
       " '_root',\n",
       " '_synset_from_pos_and_line',\n",
       " '_synset_from_pos_and_offset',\n",
       " '_synset_offset_cache',\n",
       " '_tagset',\n",
       " '_unload',\n",
       " 'abspath',\n",
       " 'abspaths',\n",
       " 'all_lemma_names',\n",
       " 'all_synsets',\n",
       " 'citation',\n",
       " 'encoding',\n",
       " 'ensure_loaded',\n",
       " 'fileids',\n",
       " 'get_version',\n",
       " 'ic',\n",
       " 'jcn_similarity',\n",
       " 'langs',\n",
       " 'lch_similarity',\n",
       " 'lemma',\n",
       " 'lemma_count',\n",
       " 'lemma_from_key',\n",
       " 'lemmas',\n",
       " 'license',\n",
       " 'lin_similarity',\n",
       " 'morphy',\n",
       " 'of2ss',\n",
       " 'open',\n",
       " 'path_similarity',\n",
       " 'readme',\n",
       " 'res_similarity',\n",
       " 'root',\n",
       " 'ss2of',\n",
       " 'synset',\n",
       " 'synsets',\n",
       " 'unicode_repr',\n",
       " 'words',\n",
       " 'wup_similarity']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# morphological substitutions, meronymies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('tree.n.01')\n",
      "[Synset('woody_plant.n.01')]\n",
      "[Synset('aalii.n.01'), Synset('acacia.n.01'), Synset('african_walnut.n.01'), Synset('albizzia.n.01'), Synset('alder.n.02'), Synset('angelim.n.01'), Synset('angiospermous_tree.n.01'), Synset('anise_tree.n.01'), Synset('arbor.n.01'), Synset('aroeira_blanca.n.01'), Synset('ash.n.02'), Synset('australian_nettle.n.01'), Synset('balata.n.02'), Synset('bayberry.n.01'), Synset('bean_tree.n.01'), Synset('beech.n.01'), Synset('birch.n.02'), Synset('bitterwood_tree.n.01'), Synset('black_mangrove.n.01'), Synset('blackwood.n.02'), Synset('bloodwood_tree.n.01'), Synset('bonduc.n.02'), Synset('bonsai.n.01'), Synset('bottle-tree.n.01'), Synset('brazilian_ironwood.n.01'), Synset('brazilian_pepper_tree.n.01'), Synset('brazilwood.n.02'), Synset('breakax.n.01'), Synset('burma_padauk.n.01'), Synset('button_tree.n.01'), Synset('cabbage_tree.n.03'), Synset('calaba.n.01'), Synset('calabash.n.02'), Synset('camwood.n.01'), Synset('caracolito.n.01'), Synset('carib_wood.n.01'), Synset('cassia.n.01'), Synset('casuarina.n.01'), Synset('chaulmoogra.n.01'), Synset('chestnut.n.02'), Synset('chinaberry.n.02'), Synset('chinese_parasol_tree.n.01'), Synset('christmas_bush.n.01'), Synset('cinchona.n.02'), Synset('clusia.n.01'), Synset('cockspur.n.02'), Synset('cocobolo.n.01'), Synset('coffee.n.02'), Synset('conacaste.n.01'), Synset('coral_tree.n.01'), Synset('coralwood.n.01'), Synset('cork_tree.n.01'), Synset('dagame.n.01'), Synset('devilwood.n.01'), Synset('dhak.n.01'), Synset('dhawa.n.01'), Synset('dipterocarp.n.01'), Synset('dita.n.01'), Synset('divi-divi.n.02'), Synset('ebony.n.03'), Synset('elm.n.01'), Synset('fever_tree.n.01'), Synset('fig_tree.n.01'), Synset('fringe_tree.n.01'), Synset('giant_chinkapin.n.01'), Synset('gliricidia.n.01'), Synset('granadilla_tree.n.01'), Synset('guama.n.01'), Synset('guinea_pepper.n.02'), Synset('gum_tree.n.01'), Synset('gutta-percha_tree.n.01'), Synset('gutta-percha_tree.n.02'), Synset('gymnospermous_tree.n.01'), Synset('hackberry.n.01'), Synset('hazel.n.01'), Synset('hop_hornbeam.n.01'), Synset('hornbeam.n.01'), Synset('hydnocarpus_laurifolia.n.01'), Synset('ice-cream_bean.n.01'), Synset('idesia.n.01'), Synset('incense_tree.n.01'), Synset('indian_beech.n.01'), Synset('inga.n.01'), Synset('ivory_tree.n.01'), Synset('jamaica_dogwood.n.01'), Synset('jamaican_cherry.n.01'), Synset('japanese_pagoda_tree.n.01'), Synset('kentucky_coffee_tree.n.01'), Synset('ketembilla.n.01'), Synset('keurboom.n.01'), Synset('keurboom.n.02'), Synset('kingwood.n.02'), Synset('kino.n.02'), Synset('kowhai.n.01'), Synset('lacebark.n.01'), Synset('lancewood.n.02'), Synset('lanseh_tree.n.01'), Synset('laurelwood.n.01'), Synset('lead_tree.n.01'), Synset('lemonwood.n.02'), Synset('lepidobotrys.n.01'), Synset('linden.n.02'), Synset('locust_tree.n.01'), Synset('mahogany.n.02'), Synset('manila_tamarind.n.01'), Synset('marblewood.n.02'), Synset('maria.n.02'), Synset('marmalade_tree.n.01'), Synset('mayeng.n.01'), Synset('mescal_bean.n.01'), Synset('millettia.n.01'), Synset('montezuma.n.01'), Synset('msasa.n.01'), Synset('nakedwood.n.01'), Synset('necklace_tree.n.01'), Synset('neem.n.01'), Synset('nitta_tree.n.01'), Synset('oak.n.02'), Synset('oak_chestnut.n.01'), Synset('obeche.n.02'), Synset('opepe.n.01'), Synset('padauk.n.01'), Synset('palm.n.03'), Synset('palo_verde.n.01'), Synset('pandanus.n.02'), Synset('pepper_tree.n.01'), Synset('pepper_tree.n.02'), Synset('peruvian_balsam.n.01'), Synset('plane_tree.n.01'), Synset('pollard.n.01'), Synset('poon.n.02'), Synset('prickly_ash.n.01'), Synset('prickly_ash.n.02'), Synset('princewood.n.01'), Synset('puka.n.02'), Synset('quandong.n.01'), Synset('quandong.n.03'), Synset('quira.n.02'), Synset('red_sandalwood.n.02'), Synset('red_silk-cotton_tree.n.01'), Synset('ribbon_tree.n.01'), Synset('rose_chestnut.n.01'), Synset('rosewood.n.02'), Synset('sandalwood_tree.n.01'), Synset('sapling.n.01'), Synset('satinwood.n.03'), Synset('scarlet_wisteria_tree.n.01'), Synset('scrub_beefwood.n.01'), Synset('shade_tree.n.01'), Synset('shaving-brush_tree.n.01'), Synset('shingle_tree.n.01'), Synset('silver_ash.n.01'), Synset('silver_tree.n.01'), Synset('silver_tree.n.02'), Synset('sissoo.n.01'), Synset('snag.n.02'), Synset('soapberry.n.01'), Synset('souari.n.01'), Synset('southern_beech.n.01'), Synset('spanish_tamarind.n.01'), Synset('tanbark_oak.n.01'), Synset('teak.n.02'), Synset('timber_tree.n.01'), Synset('tipu.n.01'), Synset('tolu_tree.n.01'), Synset('tree_of_knowledge.n.01'), Synset('treelet.n.01'), Synset('trifoliate_orange.n.01'), Synset('tulipwood_tree.n.01'), Synset('turreae.n.01'), Synset('wheel_tree.n.01'), Synset('white_mangrove.n.01'), Synset('white_mangrove.n.02'), Synset('wild_fig.n.02'), Synset('wild_medlar.n.01'), Synset('wild_tamarind.n.02'), Synset('willow.n.01'), Synset('winter's_bark.n.02'), Synset('yellowwood.n.02'), Synset('zebrawood.n.02')]\n",
      "'Synset' object has no attribute 'meronyms'\n",
      "'Synset' object has no attribute 'holonyms'\n",
      "[Synset('burl.n.02'), Synset('crown.n.07'), Synset('limb.n.02'), Synset('stump.n.01'), Synset('trunk.n.01')]\n",
      "'Synset' object has no attribute 'sisterm_terms'\n",
      "'Synset' object has no attribute 'troponyms'\n",
      "'Synset' object has no attribute 'inherited_hypernyms'\n",
      "Synset('tree.n.02')\n",
      "[Synset('plane_figure.n.01')]\n",
      "[Synset('cladogram.n.01'), Synset('stemma.n.01')]\n",
      "'Synset' object has no attribute 'meronyms'\n",
      "'Synset' object has no attribute 'holonyms'\n",
      "[]\n",
      "'Synset' object has no attribute 'sisterm_terms'\n",
      "'Synset' object has no attribute 'troponyms'\n",
      "'Synset' object has no attribute 'inherited_hypernyms'\n",
      "Synset('tree.n.03')\n",
      "[]\n",
      "[]\n",
      "'Synset' object has no attribute 'meronyms'\n",
      "'Synset' object has no attribute 'holonyms'\n",
      "[]\n",
      "'Synset' object has no attribute 'sisterm_terms'\n",
      "'Synset' object has no attribute 'troponyms'\n",
      "'Synset' object has no attribute 'inherited_hypernyms'\n",
      "Synset('corner.v.02')\n",
      "[Synset('steer.v.01')]\n",
      "[]\n",
      "'Synset' object has no attribute 'meronyms'\n",
      "'Synset' object has no attribute 'holonyms'\n",
      "[]\n",
      "'Synset' object has no attribute 'sisterm_terms'\n",
      "'Synset' object has no attribute 'troponyms'\n",
      "'Synset' object has no attribute 'inherited_hypernyms'\n",
      "Synset('tree.v.02')\n",
      "[Synset('plant.v.01')]\n",
      "[]\n",
      "'Synset' object has no attribute 'meronyms'\n",
      "'Synset' object has no attribute 'holonyms'\n",
      "[]\n",
      "'Synset' object has no attribute 'sisterm_terms'\n",
      "'Synset' object has no attribute 'troponyms'\n",
      "'Synset' object has no attribute 'inherited_hypernyms'\n",
      "Synset('tree.v.03')\n",
      "[Synset('chase.v.01')]\n",
      "[]\n",
      "'Synset' object has no attribute 'meronyms'\n",
      "'Synset' object has no attribute 'holonyms'\n",
      "[]\n",
      "'Synset' object has no attribute 'sisterm_terms'\n",
      "'Synset' object has no attribute 'troponyms'\n",
      "'Synset' object has no attribute 'inherited_hypernyms'\n",
      "Synset('tree.v.04')\n",
      "[Synset('elongate.v.01')]\n",
      "[]\n",
      "'Synset' object has no attribute 'meronyms'\n",
      "'Synset' object has no attribute 'holonyms'\n",
      "[]\n",
      "'Synset' object has no attribute 'sisterm_terms'\n",
      "'Synset' object has no attribute 'troponyms'\n",
      "'Synset' object has no attribute 'inherited_hypernyms'\n"
     ]
    }
   ],
   "source": [
    "for synset in (wn.synsets('tree')):\n",
    "        print(synset)\n",
    "        nyms = ['hypernyms', 'hyponyms', 'meronyms', 'holonyms', 'part_meronyms', 'sisterm_terms', 'troponyms', 'inherited_hypernyms']\n",
    "        for i in nyms:\n",
    "            try:\n",
    "                print(getattr(synset, i)())\n",
    "            except AttributeError as e: \n",
    "                print(e)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
